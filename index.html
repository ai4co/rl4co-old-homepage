<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VCBE0EGS1N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VCBE0EGS1N');
</script>


<!DOCTYPE html>
<html lang=""><head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>RL4CO</title>
    <meta name="description" content="RL4CO: an Extensive Reinforcement Learning for
 Combinatorial Optimization Benchmark">
    <meta property="og:title" content="RL4CO" />
    <meta property="og:description"
          content="RL4CO: an Extensive Reinforcement Learning for
 Combinatorial Optimization Benchmark" />
    <meta property="og:image" content="img/logo.png" />
    <meta name="keywords" content="" />
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
        integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
          crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
        integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
        crossorigin="anonymous">
    </script>

  <script src="https://cdn.jsdelivr.net/clipboard.js/2.0.0/clipboard.min.js"></script>


    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);">
    </script>
    
    <meta name="keywords" content="fast, hugo, theme, minimal, gruvbox">
    <link rel="icon" type="image/png" href='img/logo.png' />
    <meta name="author" content='rl4co'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.115.0">
    
    <link rel="stylesheet" href="sass/main.min.4f64d07551f36a4f65afec8f638f7020fab11dc2a21c1649746e97619f52cad2.css" type="text/css" media="screen">

    

    

    <script src="https://code.iconify.design/iconify-icon/1.0.7/iconify-icon.min.js"></script>

    
    
    <link rel="alternate" type="application/rss+xml" href="/index.xml" title="RL4CO" />
    </head>
<body>
      <div class="line" id="scrollIndicator"></div>
      <div class="main"><div class="title">
  
</div>
<script>
  const themeSetter = (theme) => {
      document.body.classList.toggle('dark')
      localStorage.setItem('theme', theme)
      blockSwitcher()
  }

  const blockSwitcher = () => [...document.getElementsByTagName("BLOCKQUOTE")]
	.forEach(b => b.classList.toggle('dark'))

  const styleSwapper = () => {
      document.body.classList.add('back-transition')
      if (localStorage.getItem('theme') === 'dark') themeSetter('light')
      else if (localStorage.getItem('theme') === 'light') themeSetter('dark')
  }

  if (localStorage.getItem('theme') === 'dark'){
      themeSetter('dark')
      document.addEventListener("DOMContentLoaded", blockSwitcher)
  }
 else localStorage.setItem('theme', 'light')

  document.getElementById('chk').addEventListener('change',styleSwapper);

  window.addEventListener("scroll", () => {
      let height = document.documentElement.scrollHeight
          - document.documentElement.clientHeight;
      if(height >= 500){
	  let winScroll = document.body.scrollTop
              || document.documentElement.scrollTop;
	  let scrolled = (winScroll / height) * 100;
	  document.getElementById("scrollIndicator").style.width = scrolled + "%";
      }
  });
</script>

<div class="intro" style="align-items: center">
  <!-- <p class="intro-title">RL4CO: an Extensive Reinforcement Learning for
 Combinatorial Optimization Benchmark</p> -->

<p class="intro-title">
  <img src="img/rl4co.png" alt="overall-figure" style="width:60%">
</p>

  <p id="intro-author">
    <a href="https://fedebotu.github.io/">Federico Berto</a><sup>* 1</sup>, 
    <a href="https://scholar.google.com/citations?user=fjKA5gYAAAAJ&hl=en">Chuanbo Hua</a><sup>* 1</sup>, 
    <a href="https://junyoungpark.github.io/">Junyoung Park</a><sup>* 1, 2</sup>,<br> 
    <a href="https://scholar.google.com/citations?user=VvyLuhAAAAAJ&hl=en&oi=sra">Minsu Kim</a><sup>1</sup>, 
    <a href="https://scholar.google.com/citations?user=TYYYjckAAAAJ&hl=en&oi=sra">Hyeonah Kim</a><sup>1</sup>, 
    <a href="https://scholar.google.com/citations?user=zHyj8zAAAAAJ&hl=en&oi=sra">Jiwoo Son</a><sup>1</sup>, 
    <a href="https://sites.google.com/view/haeyeon-rachel-kim/home">Haeyeon Kim</a><sup>1</sup>, 
    <a href="https://scholar.google.com/citations?hl=en&user=92PvccwAAAAJ">Joungho Kim</a><sup>1</sup>, 
    <a href="https://scholar.google.com/citations?user=sH2a0nkAAAAJ&hl=en&oi=sra">Jinkyoo Park</a><sup>1, 2</sup><br>
    <sup>1</sup> Korea Advanced Institute of Science and Technology (KAIST)<br>
    <sup>2</sup> OMELET AI
  </p>
</div>
<div class="nav">
    <a href="https://join.slack.com/t/rl4co/shared_invite/zt-1ytz2c1v4-0IkQ8NQH4TRXIX8PrRmDhQ">
      <iconify-icon icon="mdi:slack" class="icon"></iconify-icon>
      Slack
    </a>
    <a href="https://arxiv.org/abs/2306.17100">
      <iconify-icon icon="academicons:arxiv" class="icon"></iconify-icon>
      arXiv
    </a>
    <a href="https://github.com/kaist-silab/rl4co">
      <iconify-icon icon="mdi:github" class="icon"></iconify-icon>
      GitHub
    </a>
    <a href="https://rl4co.readthedocs.io/en/latest/">
      <iconify-icon icon="simple-line-icons:docs" class="icon"></iconify-icon>
      Docs
    </a>
    <a href="https://github.com/kaist-silab/rl4co/tree/main/rl4co/models/zoo">
      <iconify-icon icon="carbon:model-alt" class="icon"></iconify-icon>
      Models
    </a>
    <a href="https://github.com/kaist-silab/rl4co/tree/main/rl4co/envs">
      <iconify-icon icon="material-symbols:rebase" class="icon"></iconify-icon>
      Envs
    </a>
</div>


An extensive Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.

RL4CO is built upon:
<ul>
  <li><a href="https://github.com/pytorch/rl">TorchRL</a>: official PyTorch framework for RL algorithms and vectorized environments on GPUs</li>
  <li><a href="https://github.com/pytorch-labs/tensordict">TensorDict</a>: a library to easily handle heterogeneous data such as states, actions and rewards</li>
  <li><a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>: a lightweight PyTorch wrapper for high-performance AI research</li>
  <li><a href="https://hydra.cc/">Hydra</a>: a framework for elegantly configuring complex applications</li>
</ul>


<h2>Installation</h2>

You can easily install RL4CO from PyPI:
<!-- <pre> -->
    <code id="pip">
        pip install rl4co
    </code>
<!-- </pre> -->


<h2>Unified and Modular Implementation of RL-for-CO</h2>


<div class="overall-figure" style="align-items: center">
  <img src="img/overview.png" alt="overall-figure" style="width:100%">
</div>

<p id="text-before-toggle">RL4CO aims to decouple the major components of the autoregressive policy of NCO and its training
routine while prioritizing reusability. We consider five major components, which are explained in
the following paragraphs.</p>

<div class="toggle-box">
  <input type="checkbox" id="box-1">
  <h3><label for="box-1">Policy</label></h3>
  <p class="toggle-text">
    This module takes the problem and constructs solutions autoregressively. The policy consists of the following components: 
    <span class="codetext">Init Embedding</span>, <span class="codetext">Encoder</span>, <span class="codetext">Context Embedding</span>, and <span class="codetext">Decoder</span>. Each of these components is 
    designed as an independent module for easy integration. Our policy design is flexible enough to reimplement state-of-the-art 
    autoregressive policies, including AM, POMO, and Sym-NCO, for various CO problems such as TSP, CVRP, OP, PCTSP, PDP, and mTSP, 
    to name a few.
  </p>
</div>

<div class="toggle-box">
  <input type="checkbox" id="box-2">
  <h3><label for="box-2">Environment</label></h3>
  <p class="toggle-text">
    This module fully specifies the given problem and updates the problem construction steps based on the input action. 
    When implementing the <span class="codetext">environment</span>, we focus on parallel execution of rollouts (i.e., problem-solving) while 
    maintaining statelessness in updating every step of solution decoding. These features are essential for ensuring 
    the reproducibility of NCO and supporting "look-back" decoding schemes such as Monte-Carlo Tree Search. <br><br>
    Our environment implementation is based on <span class="codetext">TorchRL</span>, an open-source RL 
    library for <span class="codetext">PyTorch</span>, which aims at high modularity and good runtime performance, 
    especially on GPUs. This design choice makes the <span class="codetext">Environment</span> implementation standalone, even outside of RL4CO, 
    and consistently empowered by a community-supporting library -- <span class="codetext">TorchRL</span>.
  </p>
</div>

<div class="toggle-box">
  <input type="checkbox" id="box-3">
  <h3><label for="box-3">RL Algorithm</label></h3>
  <p class="toggle-text">
    This module defines the routine that takes the <span class="codetext">Policy</span>, <span class="codetext">Environment</span>, and problem instances and generates 
    the gradients of the policy (and possibly the critic for actor-critic methods). We intentionally decouple the routines for 
    gradient computations and parameter updates to support modern training practices, which will be explained in the next 
    paragraph.
  </p>
</div>

<div class="toggle-box">
  <input type="checkbox" id="box-4">
  <h3><label for="box-4">Trainer</label></h3>
  <p class="toggle-text">
    Training a single NCO model is typically computationally demanding, especially since most CO problems are NP-hard. 
    Therefore, implementing a modernized training routine becomes crucial. To this end, we implement the <span class="codetext">Trainer</span> 
    using <span class="codetext">Lightning</span>, which seamlessly supports features of modern training 
    pipelines, including logging, checkpoint management, automatic mixed-precision training, various hardware acceleration 
    supports (e.g., CPU, GPU, TPU, and Apple Silicon), multi-GPU support, and even multi-machine expansion. We have found
     that using mixed-precision training significantly decreases training time without sacrificing NCO solver quality and 
     enables leveraging recent routines such as FlashAttention.
  </p>
</div>

<div class="toggle-box">
  <input type="checkbox" id="box-5">
  <h3><label for="box-5">Configuration Management</label></h3>
  <p class="toggle-text">
    Optionally, but usefully, we adopt <span class="codetext">Hydra</span>, an open-source Python framework that enables 
    hierarchical config management. It promotes modularity, scalability, and reproducibility, making it easier to manage 
    complex configurations and experiments with different settings and maintain consistency across different environments. 
    An overview of RL4CO code implementation is visualized in the overview figure.
  </p>
</div>


<h2 id="title-after-toggle">Contribute</h2>


Have a suggestion, request, or found a bug? Feel free to open an issue or submit a pull request. We welcome and look forward to all contributions to RL4CO!
We are also on Slack if you have any questions or would like to discuss RL4CO with us. We are open to collaborations and would love to hear from you ðŸš€

<br>


<h2 id="title-after-toggle">Cite us</h2>
<p>If you find RL4CO valuable for your research or applied projects:</p>
<pre>
    <code id="bibtex">
        @article&lcub;berto2023rl4co,
            title = &lcub;&lcub;RL4CO&rcub;: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark&rcub;,
            author=&lcub;Federico Berto and Chuanbo Hua and Junyoung Park and Minsu Kim and Hyeonah Kim and Jiwoo Son and Haeyeon Kim and Joungho Kim and Jinkyoo Park&rcub;,
            journal=&lcub;arXiv preprint arXiv:2306.17100&rcub;,
            year=&lcub;2023&rcub;,
            url = &lcub;https://github.com/kaist-silab/rl4co&rcub;
        &rcub;
    </code>
</pre> 

<footer id="footer">
    <p class="copyright" style="color:rgb(179, 179, 179);">2023 Â© Copyright Federico Berto, Chuanbo Hua, Junyoung Park</p>
</footer>
</div>
    </body>
</html>

